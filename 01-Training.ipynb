{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916a0f48-14bb-4e74-8fa9-67c8416440b1",
   "metadata": {},
   "source": [
    "# PMA3: Making and Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d07b867-18cb-4eba-94ef-be218a77d7de",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f30196a-7884-446b-965c-13dd29a2e864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mininum Song Lenth\n",
    "# This is just to adjust the structure of the data\n",
    "min_song_length_seconds = 60\n",
    "\n",
    "# Maxinum Song Length\n",
    "max_song_length_seconds = 540\n",
    "\n",
    "# Use padding enabled files\n",
    "padding = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d40422e-dd90-4968-8828-3d3a43e86819",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "use_gpu = True\n",
    "\n",
    "# If you want to load and continue training an pre-existing model\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6fd03-a3af-435b-b7a0-feb97e041abc",
   "metadata": {},
   "source": [
    "## Loading Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dff9666-79af-4036-9d05-09a253e70a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6339271c-c0fc-4f05-b301-8ce52e249eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0abcdc87-11db-4d7c-9b05-84ac6e87be06",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for filename in tqdm(os.listdir(\"./data/padding-\" + str(padding) + \"/min-\" + str(min_song_length_seconds) + \"_max-\" + str(max_song_length_seconds))):\n",
    "        with open(\"./data/padding-\" + str(padding) + \"/min-\" + str(min_song_length_seconds) + \"_max-\" + str(max_song_length_seconds) + \"/\" + filename, \"rb\") as file:\n",
    "            json_pkl = pickle.load(file)\n",
    "\n",
    "        for frame in json_pkl[\"fmccs\"]:\n",
    "            data.append((frame, json_pkl['duration']))\n",
    "            labels.append(json_pkl['rating'])\n",
    "except:\n",
    "    raise FileNotFoundError(\"Could not read pkl files from dataset! This could happen because you have not ran preprocessing yet for your settings! Please make sure your settings are the same!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0201781b-a12c-4972-bc45-b8706057a477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(data):\n",
    "    assert data.shape[1] % 2 == 0, f\"The data cannot be shaped becuase it cannot be divided by 2 without a remainder. Data shape {data.shape}. \\\n",
    "        Failed to reshape to {data.shape[1] / 2}\"\n",
    "    \n",
    "    data = data.reshape(26, int(data.shape[1]/2))\n",
    "    data = np.expand_dims(data,axis=-1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c77a550-cbe2-4bce-88e5-3de70ffcdeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32013, 26, 1292, 1)\n"
     ]
    }
   ],
   "source": [
    "# Remember that the data is a tuple\n",
    "# (fmcc, duration)\n",
    "audio_data = np.array([reshape(x[0]) for x in data])\n",
    "print(audio_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a207ccd-480d-41b4-b385-e5dc8a017fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32013,)\n"
     ]
    }
   ],
   "source": [
    "audio_length = np.array([x[1] for x in data])\n",
    "print(audio_length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12c23e8c-d629-47b9-a999-1f32a58677ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32013, 1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape `audio_length` to match the shape of `processed_data`\n",
    "audio_length = np.reshape(audio_length, (audio_length.shape[0], 1, 1, 1))\n",
    "print(audio_length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9def3e7a-a09f-4b49-8525-c9569b5aa249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32013, 26, 1292, 1)\n"
     ]
    }
   ],
   "source": [
    "# Broadcast `audio_length` to match the shape of `processed_data`\n",
    "audio_length = np.broadcast_to(audio_length, audio_data.shape[:-1] + (1,))\n",
    "print(audio_length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258fb887-0031-4bba-8773-e241ccd3a148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32013, 26, 1292, 2)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate `processed_data` and `audio_length` along the last axis\n",
    "processed_data = np.concatenate([audio_data, audio_length], axis=-1)\n",
    "print(processed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0625815-16ac-4fd0-b274-16e7b5374dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbenc = LabelEncoder()\n",
    "labels = lbenc.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dc9ec8b-203c-4a5d-b55e-5fcf596c8da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = lbenc.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55dc05c0-c174-45ca-ab34-a714b592721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data for training and testing\n",
    "x_train,x_val,y_train,y_val = train_test_split(processed_data,labels,test_size=0.2,\n",
    "                                               shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38002f6-a427-4bf4-b4f9-1fca7fa3fae9",
   "metadata": {},
   "source": [
    "## Making Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5141cb39-6517-4fa3-b284-e3f4d7c50eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import InputLayer, Input, Conv2D , AveragePooling2D , GlobalAvgPool2D , Dense, MaxPooling2D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69d41bc8-5f8f-4547-b7a3-4cadce086977",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu is False:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ea919d3-35fc-49c6-b672-906c28bf0f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 26, 1292, 2)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 24, 1290, 256)     4864      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 22, 1288, 256)     590080    \n",
      "                                                                 \n",
      " average_pooling2d (AverageP  (None, 11, 644, 256)     0         \n",
      " ooling2D)                                                       \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 9, 642, 256)       590080    \n",
      "                                                                 \n",
      " average_pooling2d_1 (Averag  (None, 5, 321, 256)      0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 318, 512)       2097664   \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,448,202\n",
      "Trainable params: 3,448,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(26, 1292, 2))\n",
    "\n",
    "# Convolutional layers\n",
    "conv1 = Conv2D(256, (3), padding='valid', activation='relu')(input_layer)\n",
    "conv2 = Conv2D(256, (3), padding='valid', activation='relu')(conv1)\n",
    "pool1 = AveragePooling2D(pool_size=(3), strides=(2), padding='same')(conv2)\n",
    "conv3 = Conv2D(256, (3), padding='valid', activation='relu')(pool1)\n",
    "pool2 = AveragePooling2D(pool_size=(3), strides=(2), padding='same')(conv3)\n",
    "conv4 = Conv2D(512, (4), padding='valid', activation='relu')(pool2)\n",
    "global_pool = GlobalAvgPool2D()(conv4)\n",
    "\n",
    "# Classification layers\n",
    "dense1 = Dense(256, activation='relu')(global_pool)\n",
    "dense2 = Dense(128, activation='relu')(dense1)\n",
    "output = Dense(10, activation='softmax')(dense2)\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.models.Model(inputs=input_layer, outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c02e518e-d8d0-48dd-8f75-ef569f5a514b",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\music_analyzer\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\music_analyzer\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train,y_train,batch_size=batch_size,epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 (music_analyzer)",
   "language": "python",
   "name": "music_analuzer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
